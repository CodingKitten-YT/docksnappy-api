version: '3.8'

services:
  flyai-inference:
    image: flyai/flyai-server:latest  # Use the official FlyAI Server Docker image
    container_name: flyai-server
    ports:
      - "80:80"  # Expose HTTP traffic from the container to the host on port 80
      - "5000:5000"  # Expose the TensorFlow Serving API on port 5000
    volumes:
      - "./models:/app/models"  # Mount your local models directory to the container's models directory
      - "./logs:/app/logs"  # Optional: Mount a logs directory to store FlyAI logs locally
    command: --model-path /app/models --port 5000 --log-dir /app/logs

  nginx-reverse-proxy:
    image: nginx:latest
    container_name: nginx-proxy
    ports:
      - "80:80"  # Map HTTP to the host
      - "443:443"  # Map HTTPS to the host (if using SSL)
    volumes:
      - "./nginx.conf:/etc/nginx/nginx.conf"  # Mount your custom nginx configuration file
    depends_on:
      - flyai-inference

networks:
  default:
    driver: bridge

# Optional: Define a network for the services to communicate over if needed
networks:
  flyai-network:
    driver: bridge

services:
  flyai-inference:
    networks:
      - flyai-network
  nginx-reverse-proxy:
    networks:
      - flyai-network